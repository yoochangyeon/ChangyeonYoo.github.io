{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 생성 방법들"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.771466Z",
     "start_time": "2025-09-26T13:20:47.737590Z"
    }
   },
   "source": "import torch\n\n# torch.Tensor() - float32 타입으로 텐서 생성\nt1 = torch.Tensor([1, 2, 3], device='cpu')\nprint(t1.dtype)\nprint(t1.device)\nprint(t1.requires_grad)\nprint(t1.size())\nprint(t1.shape)\n\nt1_cpu = t1.cpu()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "execution_count": 130
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dtype: 텐서의 데이터 타입을 반환 (예: torch.float32, torch.int64)\n",
    "- device: 텐서가 저장된 장치를 반환 ('cpu' 또는 'cuda')\n",
    "- requires_grad: 자동 미분을 위한 그래디언트 계산 여부 (True/False)\n",
    "- size()와 shape: 텐서의 차원 크기를 반환 (동일한 정보 제공)\n",
    "- cpu(): 텐서를 CPU로 이동"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.801477Z",
     "start_time": "2025-09-26T13:20:47.797524Z"
    }
   },
   "source": "# torch.tensor() - 자동 타입 추론으로 텐서 생성\nt2 = torch.tensor([1, 2, 3], device='cpu')\nprint(t2.dtype)\nprint(t2.device)\nprint(t2.requires_grad)\nprint(t2.size())\nprint(t2.shape)\n\nt2_cpu = t2.cpu()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "execution_count": 131
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자동 타입 추론: 입력 데이터의 타입을 자동으로 추론 (정수 → int64, 실수 → float32)\n",
    "- to(): 텐서를 다른 장치로 이동 (예: .to(torch.device('cuda')))\n",
    "- cuda(): 텐서를 GPU로 이동하는 편리한 메서드"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.853517Z",
     "start_time": "2025-09-26T13:20:47.846270Z"
    }
   },
   "source": "# 다차원 텐서의 형태 확인 - 대괄호 중첩 수준이 차원 결정\na1 = torch.tensor(1)\nprint(a1.shape, a1.ndim)\n\na2 = torch.tensor([1])\nprint(a2.shape, a2.ndim)\n\na3 = torch.tensor([1, 2, 3, 4, 5])\nprint(a3.shape, a3.ndim)\n\na4 = torch.tensor([[1], [2], [3], [4], [5]])\nprint(a4.shape, a4.ndim)\n\na5 = torch.tensor([\n    [1, 2],\n    [3, 4],\n    [5, 6]\n])\nprint(a5.shape, a5.ndim)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ndim 속성: 텐서의 차원 수를 반환\n",
    "- 중요: 대괄호 []의 중첩 수준이 텐서의 차원을 결정합니다"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.887772Z",
     "start_time": "2025-09-26T13:20:47.884346Z"
    }
   },
   "source": [
    "a6 = torch.tensor([\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n"
     ]
    }
   ],
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.909602Z",
     "start_time": "2025-09-26T13:20:47.906552Z"
    }
   },
   "source": [
    "a9 = torch.tensor([\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동일한 데이터도 중첩 구조에 따라 다른 shape를 가질 수 있습니다\n",
    "- 배치 처리시 첫 번째 차원이 일반적으로 배치 크기를 나타냅니다\n",
    "- 차원별 의미를 명확히 정의하여 일관성을 유지하는 것이 중요합니다"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.929987Z",
     "start_time": "2025-09-26T13:20:47.925261Z"
    }
   },
   "source": [
    "# a11 = torch.tensor([\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "# ])\n"
   ],
   "outputs": [],
   "execution_count": 135
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동일한 차원에서 모든 요소의 길이가 일치해야 합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b_tensor_initialization_copy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메모리 공유와 복사"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.977348Z",
     "start_time": "2025-09-26T13:20:47.950937Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "execution_count": 136
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.Tensor(): 데이터를 float32로 복사하여 새로운 텐서 생성\n",
    "- torch.tensor(): 데이터 타입을 자동 추론하여 복사본 생성\n",
    "- torch.as_tensor(): 데이터와 메모리를 공유하여 텐서 생성\n",
    "- 중요: Python 리스트의 경우 세 방법 모두 복사본을 생성하므로 원본 수정이 텐서에 영향을 주지 않습니다"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:47.989078Z",
     "start_time": "2025-09-26T13:20:47.983815Z"
    }
   },
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([100,   2,   3])\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.Tensor(): NumPy 배열을 float32로 복사하여 독립적인 텐서 생성\n",
    "- torch.tensor(): NumPy 배열을 복사하여 독립적인 텐서 생성\n",
    "- torch.as_tensor(): NumPy 배열과 메모리를 공유하여 텐서 생성\n",
    "- torch.as_tensor()로 NumPy 배열에서 생성한 텐서는 원본 배열과 메모리를 공유하므로, 한쪽을 수정하면 다른 쪽도 함께 변경됩니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c_tensor_initialization_constant_values.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상수값 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.022343Z",
     "start_time": "2025-09-26T13:20:48.016799Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)\n",
    "print(t1_like)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "execution_count": 138
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.ones(): 지정된 크기의 텐서를 1로 채워서 생성\n",
    "- torch.ones_like(): 기존 텐서와 동일한 크기와 타입의 텐서를 1로 채워서 생성\n",
    "- size 매개변수: 튜플 또는 단일 정수를 전달할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.039356Z",
     "start_time": "2025-09-26T13:20:48.035370Z"
    }
   },
   "source": [
    "t2 = torch.zeros(size=(6,))\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)\n",
    "print(t2_like)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.zeros(): 지정된 크기의 텐서를 0으로 채워서 생성\n",
    "- torch.zeros_like(): 기존 텐서와 동일한 크기와 타입의 텐서를 0으로 채워서 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.054432Z",
     "start_time": "2025-09-26T13:20:48.051627Z"
    }
   },
   "source": [
    "t3 = torch.empty(size=(4,))\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)\n",
    "print(t3_like)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.empty(): 지정된 크기의 텐서를 생성하지만 값을 초기화하지 않음\n",
    "- torch.empty_like(): 기존 텐서와 동일한 크기와 타입의 비어있는 텐서를 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.074816Z",
     "start_time": "2025-09-26T13:20:48.070170Z"
    }
   },
   "source": [
    "t4 = torch.eye(n=3)\n",
    "print(t4)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "execution_count": 141
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.eye(): n×n 크기의 단위행렬(Identity Matrix)을 생성\n",
    "- 단위행렬: 대각선은 1, 나머지는 0인 정방행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d_tensor_initialization_random_values.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 랜덤 텐서 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.089859Z",
     "start_time": "2025-09-26T13:20:48.086724Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 13]])\n"
     ]
    }
   ],
   "execution_count": 142
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.randint(): 지정된 범위의 정수 난수로 채워진 텐서를 생성\n",
    "- low: 최솟값 (포함)\n",
    "- high: 최댓값 (미포함)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.105610Z",
     "start_time": "2025-09-26T13:20:48.102879Z"
    }
   },
   "source": [
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6879, 0.7934, 0.8877]])\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.rand(): 0과 1 사이의 균등분포 난수로 채워진 텐서를 생성\n",
    "- 균등분포: 모든 값이 동일한 확률로 나타나는 분포"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.119499Z",
     "start_time": "2025-09-26T13:20:48.116578Z"
    }
   },
   "source": [
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3016, -0.3851,  0.3037]])\n"
     ]
    }
   ],
   "execution_count": 144
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.randn(): 평균 0, 표준편차 1인 표준 정규분포 난수로 채워진 텐서를 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.130622Z",
     "start_time": "2025-09-26T13:20:48.128143Z"
    }
   },
   "source": [
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.8675, 10.1110],\n",
      "        [10.3173,  9.3774],\n",
      "        [ 8.4790, 11.2901]])\n"
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.normal(): 사용자가 지정한 평균과 표준편차를 가진 정규분포 난수 텐서를 생성\n",
    "- mean: 분포의 중심값\n",
    "- std: 분포의 퍼짐 정도"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.147621Z",
     "start_time": "2025-09-26T13:20:48.144008Z"
    }
   },
   "source": [
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 2.5000, 5.0000])\n"
     ]
    }
   ],
   "execution_count": 146
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.linspace(): 시작값과 끝값 사이를 등간격으로 나눈 값들로 텐서를 생성\n",
    "- start: 시작값 (포함)\n",
    "- end: 끝값 (포함)\n",
    "- steps: 생성할 값의 개수"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.171218Z",
     "start_time": "2025-09-26T13:20:48.168601Z"
    }
   },
   "source": [
    "t6 = torch.arange(5)\n",
    "print(t6)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 147
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.arange(): 0부터 지정된 값까지 연속된 정수로 텐서를 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.209726Z",
     "start_time": "2025-09-26T13:20:48.202710Z"
    }
   },
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.manual_seed(): 난수 생성기의 시드값을 설정하여 재현 가능한 결과를 얻음\n",
    "- 재현성: 동일한 시드로 설정하면 같은 난수 시퀀스가 생성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e_tensor_type_conversion.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 타입 변환"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.229516Z",
     "start_time": "2025-09-26T13:20:48.227667Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "execution_count": 149
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch 함수들은 기본적으로 torch.float32 타입으로 텐서를 생성"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.242972Z",
     "start_time": "2025-09-26T13:20:48.240103Z"
    }
   },
   "source": [
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dtype 매개변수: 텐서 생성 시 데이터 타입을 명시적으로 지정"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.263600Z",
     "start_time": "2025-09-26T13:20:48.261574Z"
    }
   },
   "source": [
    "d = b.to(torch.int32)\n",
    "print(d)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tensor.to(): 기존 텐서를 새로운 데이터 타입으로 변환\n",
    "- 새로운 텐서 반환: 원본 텐서는 변경되지 않고 새로운 텐서를 반환"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.281652Z",
     "start_time": "2025-09-26T13:20:48.278919Z"
    }
   },
   "source": [
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n",
      "torch.int16\n"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성 시 지정: `torch.ones(10, 2, dtype=torch.double)`\n",
    "- 메서드 체이닝: `.double()`, `.short()` 등의 편리한 메서드 사용\n",
    "- to() 메서드: `.to(torch.double)` 또는 `.to(dtype=torch.short)`\n",
    "- type() 메서드: `.type(torch.double)` - 주의: 사용 시 공백 없이 작성해야 함"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.295151Z",
     "start_time": "2025-09-26T13:20:48.292984Z"
    }
   },
   "source": [
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "execution_count": 153
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 타입 승격(Type Promotion): 서로 다른 타입의 텐서를 연산할 때 PyTorch가 자동으로 적절한 타입을 결정\n",
    "- 우선순위: 일반적으로 더 정밀한 타입(double > float > int)으로 결과가 결정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f_tensor_operations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 텐서 연산"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.308215Z",
     "start_time": "2025-09-26T13:20:48.304665Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.add(): 두 텐서의 원소별 덧셈을 수행하는 함수\n",
    "- + 연산자: torch.add()와 동일한 기능을 제공하는 편리한 문법\n",
    "- Element-wise 연산: 같은 위치의 원소끼리 덧셈이 수행됨\n",
    "- Broadcasting: 서로 다른 크기의 텐서도 특정 조건하에 연산이 가능\n",
    "- 주의사항: 연산을 위해서는 두 텐서의 크기가 호환되어야 함"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.321259Z",
     "start_time": "2025-09-26T13:20:48.318462Z"
    }
   },
   "source": [
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.sub(): 두 텐서의 원소별 뺄셈을 수행하는 함수\n",
    "- '-' 연산자: torch.sub()와 동일한 기능을 제공하는 편리한 문법"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.344394Z",
     "start_time": "2025-09-26T13:20:48.341812Z"
    }
   },
   "source": [
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.mul(): 두 텐서의 원소별 곱셈을 수행하는 함수\n",
    "- '*' 연산자: torch.mul()와 동일한 기능을 제공하는 편리한 문법\n",
    "- 행렬의 행렬곱이 아닌 같은 위치 원소끼리의 곱셈"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.371840Z",
     "start_time": "2025-09-26T13:20:48.369114Z"
    }
   },
   "source": [
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.div(): 두 텐서의 원소별 나눗셈을 수행하는 함수\n",
    "- '/' 연산자: torch.div()와 동일한 기능을 제공하는 편리한 문법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g_tensor_operations_mm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행렬 곱셈 연산"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.385403Z",
     "start_time": "2025-09-26T13:20:48.382225Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.dot(): 두 1차원 텐서(벡터)의 내적(Dot Product)을 계산\n",
    "- 스칼라 결과: 내적의 결과는 항상 스칼라 값(0차원 텐서)\n",
    "- 1차원 제한: 오직 1차원 텐서에서만 사용할 수 있음\n",
    "- 계산 공식: [a1, a2] dot [b1, b2] = a1*b1 + a2*b2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.404523Z",
     "start_time": "2025-09-26T13:20:48.398827Z"
    }
   },
   "source": [
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.mm(): 두 2차원 텐서(행렬)의 행렬 곱셈을 수행\n",
    "- 차원 조건: 첫 번째 행렬의 열 수와 두 번째 행렬의 행 수가 일치해야 함\n",
    "- 결과 크기: (m×n) × (n×k) → (m×k) 크기의 행렬이 생성됨"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.413421Z",
     "start_time": "2025-09-26T13:20:48.409951Z"
    }
   },
   "source": [
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.bmm(): 3차원 텐서에서 배치 행렬 곱셈을 수행\n",
    "- 배치 처리: 여러 개의 행렬 곱셈을 동시에 처리할 수 있음\n",
    "- 차원 구조: (b×m×n) × (b×n×k) → (b×m×k) 크기의 텐서가 생성됨\n",
    "- 배치 크기: 첫 번째 차원(배치 크기)은 두 텐서에서 동일해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h_tensor_operations_matmul.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.matmul을 이용한 범용 행렬 곱셈"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.427348Z",
     "start_time": "2025-09-26T13:20:48.421493Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())\n",
    "\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())\n",
    "\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())\n",
    "\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "execution_count": 161
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.matmul(): 입력 텐서의 차원에 따라 자동으로 적절한 행렬 곱셈을 수행하는 범용 함수\n",
    "- 벡터 x 벡터: 1D x 1D → 내적(dot product) 연산 수행, 스칼라 결과\n",
    "- 행렬 x 벡터: 2D x 1D → 행렬-벡터 곱셈, 브로드캐스팅으로 각 행과 벡터의 내적\n",
    "- 배치 행렬 x 벡터: 3D x 1D → 배치별로 행렬-벡터 곱셈 수행\n",
    "- 배치 행렬 x 배치 행렬: 3D x 3D → 배치별 행렬 곱셈(bmm과 동일)\n",
    "- 배치 행렬 x 행렬: 3D x 2D → 브로드캐스팅으로 각 배치에 동일한 행렬 곱셈 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# i_tensor_broadcasting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 브로드캐스팅 규칙과 응용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.438390Z",
     "start_time": "2025-09-26T13:20:48.435749Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "execution_count": 162
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 브로드캐스팅: 텐서의 모든 원소에 스칼라 값을 확장하여 연산을 수행\n",
    "- PyTorch가 자동으로 스칼라 값을 텐서의 크기에 맞게 확장\n",
    "- 예시: `t1 * 2.0`은 `t1 * torch.tensor([2.0, 2.0, 2.0])`와 동일하게 동작"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.460839Z",
     "start_time": "2025-09-26T13:20:48.458491Z"
    }
   },
   "source": [
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 차원 확장: 1D 텐서 `t4`가 2D 텐서 `t3`의 각 행에 맞게 확장(복사)되어 연산됨\n",
    "- 연산 과정: `t3`의 각 행 `[0, 1]`, `[2, 4]`, `[10, 10]`에 대해 `t4` `[4, 5]`가 각각 뺄셈 연산됨\n",
    "- 결과: `[[0-4, 1-5], [2-4, 4-5], [10-4, 10-5]]` → `[[-4, -4], [-2, -1], [6, 5]]`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.475091Z",
     "start_time": "2025-09-26T13:20:48.471985Z"
    }
   },
   "source": [
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)\n",
    "print(t5 - 2.0)\n",
    "print(t5 * 2.0)\n",
    "print(t5 / 2.0)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.487295Z",
     "start_time": "2025-09-26T13:20:48.485427Z"
    }
   },
   "source": [
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "execution_count": 165
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수에 스칼라 값을 곱하거나 나누는 경우에도 브로드캐스팅이 적용되어 편리하게 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.519379Z",
     "start_time": "2025-09-26T13:20:48.515940Z"
    }
   },
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])\n",
    "t8 = torch.tensor([[3, 1]])\n",
    "t9 = torch.tensor([[5], [2]])\n",
    "t10 = torch.tensor([7])\n",
    "print(t7 + t8)\n",
    "print(t7 + t9)\n",
    "print(t8 + t9)\n",
    "print(t7 + t10)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- t7(2,2) + t8(1,2): `t8`의 행이 2로 확장되어 `[[3,1],[3,1]]`이 된 후 더해집니다.\n",
    "- t7(2,2) + t9(2,1): `t9`의 열이 2로 확장되어 `[[5,5],[2,2]]`가 된 후 더해집니다.\n",
    "- t8(1,2) + t9(2,1): `t8`의 행이 2로, `t9`의 열이 2로 확장되어 `[[3,1],[3,1]]` + `[[5,5],[2,2]]` = `[[8,6],[5,3]]`이 됩니다.\n",
    "- t7(2,2) + t10(1): `t10`이 스칼라처럼 `[[7,7],[7,7]]`로 확장되어 더해집니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.531654Z",
     "start_time": "2025-09-26T13:20:48.528920Z"
    }
   },
   "source": [
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)\n",
    "print((t17 + t18).size())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "execution_count": 167
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오른쪽 정렬: 텐서의 차원을 오른쪽부터 비교\n",
    "- 규칙 1 (차원 크기 일치): 해당 차원의 크기가 같으면 통과\n",
    "- 규칙 2 (차원 크기 1): 해당 차원의 크기가 1이면 다른 텐서의 크기에 맞게 확장\n",
    "- 규칙 3 (차원 부재): 한 텐서에 차원이 없으면, 새 차원을 추가하고 규칙 2를 적용\n",
    "- 예시 (t17+t18): `(5,3,4,1)` + `(3,1,1)` → `(5,3,4,1)` + `(1,3,1,1)` (차원 부재, 오른쪽 정렬) → `(5,3,4,1)` + `(5,3,4,1)` (확장) → 결과 `(5,3,4,1)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.560420Z",
     "start_time": "2025-09-26T13:20:48.554173Z"
    }
   },
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.579828Z",
     "start_time": "2025-09-26T13:20:48.578416Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 규칙 위반: 차원을 오른쪽부터 비교했을 때, 크기가 서로 다르면서 어느 한쪽도 1이 아닌 차원이 존재하면 `RuntimeError`가 발생"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.608653Z",
     "start_time": "2025-09-26T13:20:48.604469Z"
    }
   },
   "source": [
    "t27 = torch.ones(4) * 5\n",
    "print(t27)\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)\n",
    "\n",
    "exp = torch.arange(1., 5.)\n",
    "a = torch.arange(1., 5.)\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.pow(): 거듭제곱 연산에서도 브로드캐스팅이 적용됩니다.\n",
    "- 스칼라 를 넣었을 때: `torch.pow(t27, 2)`는 텐서의 모든 원소를 2제곱합니다.\n",
    "- 텐서를 넣었을 때: `torch.pow(a, exp)`는 `a`와 `exp`의 원소별로 거듭제곱을 수행합니다. (e.g., `1**1`, `2**2`, `3**3`, `4**4`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# j_tensor_indexing_slicing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 인덱싱 및 슬라이싱"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.684930Z",
     "start_time": "2025-09-26T13:20:48.681479Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])\n",
    "print(x[:, 1])\n",
    "print(x[1, 2])\n",
    "print(x[:, -1])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `x[1]`: 1번 인덱스 행(두 번째 행) 전체를 선택\n",
    "- `x[:, 1]`: 모든 행(`:`)의 1번 인덱스 열(두 번째 열)을 선택\n",
    "- `x[1, 2]`: 1번 행, 2번 열의 특정 원소를 선택\n",
    "- `x[:, -1]`: 음수 인덱스를 사용하여 마지막 열을 선택 (NumPy와 동일한 방식)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.758678Z",
     "start_time": "2025-09-26T13:20:48.755397Z"
    }
   },
   "source": [
    "print(x[1:])\n",
    "print(x[1:, 3:])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `x[1:]`: 1번 인덱스 행부터 끝까지 모든 행을 선택합니다.\n",
    "- `x[1:, 3:]`: 1번 인덱스 행부터 끝까지, 그리고 3번 인덱스 열부터 끝까지의 부분 행렬을 선택합니다.\n",
    "- `start:end`: `start` 인덱스는 포함되고 `end` 인덱스는 포함되지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.781697Z",
     "start_time": "2025-09-26T13:20:48.778982Z"
    }
   },
   "source": [
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `y[1:4, 2] = 1`: 1번부터 3번 행까지의 2번 열에 스칼라 값 1을 할당합니다. 이 때 브로드캐스팅이 적용됩니다.\n",
    "- 부분 텐서 추출: 값을 할당한 후, 해당 부분을 다시 슬라이싱하여 변경된 내용을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.813336Z",
     "start_time": "2025-09-26T13:20:48.809466Z"
    }
   },
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print(z[1:, 1:3])\n",
    "print(z[:, 1:])\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `z[1:, 1:3] = 0`: 1번 행부터 끝까지, 그리고 1번 열부터 2번 열까지의 영역을 선택하여 모든 원소를 0으로 변경\n",
    "- 메모리 공유: 인덱싱/슬라이싱으로 얻은 텐서는 원본 텐서의 메모리를 공유하는 뷰(View)인 경우가 많음\n",
    "- 주의: `clone()`이나 `detach()`를 사용하지 않는 한, 슬라이싱된 텐서의 수정은 원본에 영향을 줄 수 있음을 항상 인지해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k_tensor_reshaping.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 형태 변경과 차원 조작"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.840382Z",
     "start_time": "2025-09-26T13:20:48.835016Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)\n",
    "t3 = t1.reshape(1, 6)\n",
    "print(t2)\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)\n",
    "print(t4)\n",
    "print(t5)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `view()`: 텐서의 형태를 변경하여 새로운 뷰(View)를 반환. 원본 텐서와 메모리를 공유\n",
    "- `reshape()`: `view()`와 유사하지만, 메모리 구조가 연속적이지 않을 경우(e.g., `transpose` 후) 복사본을 생성할 수도 있어 더 유연하고 안전\n",
    "- -1 사용: `t1.view(3, -1)`과 같이 -1을 사용하면 다른 차원으로부터 크기를 자동으로 유추\n",
    "- 메모리 공유: 두 메서드 모두 가능한 한 원본 데이터의 메모리를 공유하므로, 뷰를 변경하면 원본도 변경될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.853040Z",
     "start_time": "2025-09-26T13:20:48.849632Z"
    }
   },
   "source": [
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "t7 = t6.squeeze()\n",
    "\n",
    "t8 = t6.squeeze(0)\n",
    "print(t7)\n",
    "print(t8)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `squeeze()`: 텐서의 차원 중 크기가 1인 차원을 제거. 인자를 주지 않으면 모든 1인 차원을 제거하고, 인자를 주면 해당 위치의 1인 차원만 제거\n",
    "- `unsqueeze()`: 지정된 위치에 크기가 1인 새로운 차원을 추가\n",
    "- 활용: 모델의 입력/출력 형태를 맞추거나 브로드캐스팅을 위해 차원을 조작할 때 매우 유용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:48.905113Z",
     "start_time": "2025-09-26T13:20:48.899034Z"
    }
   },
   "source": [
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "t10 = t9.unsqueeze(1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)\n",
    "print(t12, t12.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `flatten()`: 다차원 텐서를 1차원 텐서로 펼칩니다.\n",
    "- `start_dim`: 평탄화를 시작할 차원을 지정할 수 있습니다. `start_dim=1`은 0번 차원(배치 차원)은 유지하고 나머지만 1차원으로 펼치라는 의미로, FCN(Fully Connected Network)의 입력으로 전달할 때 자주 사용됩니다.\n",
    "- 복사본 생성: 일반적으로 새로운 메모리에 데이터를 복사하여 1차원 텐서를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.094139Z",
     "start_time": "2025-09-26T13:20:49.086516Z"
    }
   },
   "source": [
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "t14 = t13.flatten()\n",
    "\n",
    "print(t14)\n",
    "\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "execution_count": 177
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `permute()`: 텐서의 차원 순서를 사용자가 지정한 순서대로 재배열합니다.\n",
    "- `dims`: `(2, 0, 1)`은 기존의 2번 차원을 0번으로, 0번 차원을 1번으로, 1번 차원을 2번으로 옮기라는 의미입니다.\n",
    "- 이미지 데이터 처리: 이미지 텐서의 차원 순서(e.g., `(Batch, Channel, Height, Width)` ↔ `(Batch, Height, Width, Channel)`)를 변경할 때 매우 유용합니다.\n",
    "- 뷰(View) 반환: `permute`는 원본 텐서의 메모리를 공유하는 뷰를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.162959Z",
     "start_time": "2025-09-26T13:20:49.151191Z"
    }
   },
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)\n",
    "print(torch.permute(t18, (2, 0, 1)).size())\n",
    "\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "t20 = torch.permute(t19, dims=(0, 1))\n",
    "t21 = torch.permute(t19, dims=(1, 0))\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "t22 = torch.transpose(t19, 0, 1)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)\n",
    "\n",
    "print(t23)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "execution_count": 178
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `transpose(dim0, dim1)`: 지정된 두 개의 차원(`dim0`, `dim1`)을 서로 맞바꿉니다.\n",
    "- `t()`: 2차원 이하의 텐서에서 0번과 1번 차원을 맞바꾸는 `transpose(0, 1)`의 축약형입니다. 3차원 이상에서는 사용할 수 없습니다.\n",
    "- 관계: `permute`는 모든 차원의 순서를 한 번에 바꿀 수 있는 더 일반적인 기능이며, `transpose`는 두 차원만 바꾸는 특수한 경우입니다.\n",
    "- 뷰(View) 반환: `transpose`와 `t` 역시 원본 텐서의 메모리를 공유하는 뷰를 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l_tensor_concat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 결합"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.208820Z",
     "start_time": "2025-09-26T13:20:49.200995Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.cat()`: 텐서들의 리스트를 입력받아 특정 차원(`dim`)을 기준으로 이어 붙임\n",
    "- 결합 조건: `dim`으로 지정된 차원을 제외한 나머지 모든 차원의 크기가 정확히 일치해야 함\n",
    "- 예시: 위 코드에서 `t1`, `t2`, `t3`는 `dim=1`을 제외한 0번, 2번 차원의 크기가 각각 2와 3으로 동일\n",
    "- 결과 크기: `dim=1` 차원의 크기는 각 텐서의 크기를 더한 값(1 + 3 + 2 = 6)이 됨. 최종 shape: `(2, 6, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.256882Z",
     "start_time": "2025-09-26T13:20:49.243593Z"
    }
   },
   "source": [
    "t5 = torch.arange(0, 3)\n",
    "t6 = torch.arange(3, 8)\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)\n",
    "print(t7)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "execution_count": 180
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차원 텐서는 차원이 하나뿐이므로 `dim=0`으로만 결합할 수 있습니다.\n",
    "- 단순히 두 텐서의 원소를 순서대로 이어 붙인 새로운 1차원 텐서를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.322449Z",
     "start_time": "2025-09-26T13:20:49.304891Z"
    }
   },
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)\n",
    "\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())\n",
    "print(t16)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "execution_count": 181
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim=0`: 행(row) 방향으로 텐서를 쌓습니다. (수직 결합) 열의 개수(3)가 동일해야 합니다.\n",
    "- `dim=1`: 열(column) 방향으로 텐서를 이어 붙입니다. (수평 결합) 행의 개수(2)가 동일해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.382549Z",
     "start_time": "2025-09-26T13:20:49.372449Z"
    }
   },
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())\n",
    "print(t19)\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())\n",
    "print(t20)\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())\n",
    "print(t21)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "execution_count": 182
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2차원과 동일한 원리가 적용됩니다.\n",
    "- `dim=0`: 첫 번째 차원(배치)을 따라 결합합니다. `(1,2,3)`과 `(1,2,3)`이 결합하여 `(2,2,3)`이 됩니다.\n",
    "- `dim=1`: 두 번째 차원(채널 또는 높이)을 따라 결합합니다. `(1,2,3)`과 `(1,2,3)`이 결합하여 `(1,4,3)`이 됩니다.\n",
    "- `dim=2`: 세 번째 차원(너비)을 따라 결합합니다. `(1,2,3)`과 `(1,2,3)`이 결합하여 `(1,2,6)`이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# m_tensor_stacking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 쌓기"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.483392Z",
     "start_time": "2025-09-26T13:20:49.469156Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))\n",
    "\n",
    "t9 = torch.arange(0, 3)\n",
    "t10 = torch.arange(3, 6)\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())\n",
    "print(t11)\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())\n",
    "print(t13)\n",
    "\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 183
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.stack()`: 동일한 크기의 텐서들을 새로운 차원으로 쌓아 결합\n",
    "- 차원 증가: 결과 텐서의 차원 수는 원본 텐서보다 1만큼 커짐\n",
    "- `dim`: 새로운 차원이 삽입될 위치를 지정. `dim=0`은 가장 바깥쪽에, `dim=1`은 그 안쪽에 새 차원을 만듦\n",
    "- 결합 조건: `stack`을 적용할 모든 텐서의 크기(shape)가 완전히 동일해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.stack(tensors, dim)`은 내부적으로 각 텐서에 `unsqueeze(dim)`을 적용하여 차원을 늘린 후, 해당 `dim`을 기준으로 `torch.cat()`을 수행하는 것과 완벽하게 동일\n",
    "- 예시: `t1` `(2,3)` → `t1.unsqueeze(0)` `(1,2,3)`. `t2`도 동일하게 변환 후 `dim=0`으로 `cat`하면 `(2,2,3)`이 됨. 이는 `torch.stack([t1, t2], dim=0)`의 결과와 같음\n",
    "- 개념적 이해: 이 관계를 이해하면 `stack`이 왜 새로운 차원을 생성하는지 명확하게 파악할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dim=0`: 1D 텐서(벡터)들을 행으로 간주하여 새로운 2D 텐서(행렬)를 만듭니다.\n",
    "- `dim=1`: 1D 텐서(벡터)들을 열로 간주하여 새로운 2D 텐서(행렬)를 만듭니다.\n",
    "- 활용: 여러 개의 1차원 샘플 데이터를 하나의 배치(2D 텐서)로 묶을 때 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n_tensor_vstack_hstack.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수직/수평 텐서 결합"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-26T13:20:49.640018Z",
     "start_time": "2025-09-26T13:20:49.619050Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "print(t6)\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "\n",
    "print(t9)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "\n",
    "print(t18)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `vstack`: 텐서들을 수직(Vertical)으로, 즉 첫 번째 차원을 따라 쌓음\n",
    "- 편의 함수: `torch.cat(tensors, dim=0)`과 동일하게 동작\n",
    "- 1D 텐서 처리: 1D 텐서 `(N,)`는 `vstack`에 의해 `(1, N)`으로 취급된 후 `cat(dim=0)`이 적용됨. 따라서 `(2, N)` 형태의 2D 텐서가 됨\n",
    "- N-D 텐서 처리: 2차원 이상의 텐서들은 첫 번째 차원을 기준으로 결합됨. `(2, 2, 3)` 텐서 두 개를 `vstack`하면 `(4, 2, 3)` 텐서가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `hstack`: 텐서들을 수평(Horizontal)으로 결합\n",
    "- 동작 방식의 차이:\n",
    "    - 1D 텐서: `torch.cat(tensors, dim=0)`과 동일하게 동작하여 하나의 긴 1D 텐서로 만듦\n",
    "    - 2D 이상 텐서: `torch.cat(tensors, dim=1)`과 동일하게 동작하여 두 번째 차원을 기준으로 결합\n",
    "- 직관성: NumPy 사용자에게 익숙한 인터페이스를 제공하여 코드의 가독성을 높여줌"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 숙제 후기\n",
    "\n",
    "PyTorch의 기본 텐서 연산들을 실습해보면서 다음과 같은 점들을 깨달았습니다:\n",
    "\n",
    "1. **텐서 생성 방법의 차이점**: `torch.Tensor()`와 `torch.tensor()`의 차이가 생각보다 중요하다는 것을 알게 되었습니다. 특히 데이터 타입 추론과 메모리 공유 부분에서 주의해야 할 점들이 많았습니다.\n",
    "\n",
    "2. **브로드캐스팅의 편리함**: 서로 다른 크기의 텐서도 자동으로 확장되어 연산이 가능한 브로드캐스팅 기능이 매우 유용했습니다. 하지만 예상치 못한 결과가 나올 수 있어서 항상 결과의 shape을 확인하는 습관이 필요하다고 느꼈습니다.\n",
    "\n",
    "3. **차원 조작의 중요성**: `view()`, `reshape()`, `squeeze()`, `unsqueeze()` 등의 함수들이 딥러닝에서 얼마나 자주 사용될지 예상이 됩니다. 특히 배치 처리를 위한 차원 조작이 핵심이라는 것을 알게 되었습니다.\n",
    "\n",
    "실습을 통해 PyTorch의 기초를 탄탄히 다질 수 있었고, 앞으로 더 복잡한 모델을 구현할 때 도움이 될 것 같습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
